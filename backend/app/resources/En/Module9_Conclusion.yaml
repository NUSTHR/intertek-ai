module_id: 9 #"09_compliance_conclusions"
title: "Final Compliance Determination"
description: "Maps parameter states to final legal conclusions and comprehensive obligation views based on Regulation (EU) 2024/1689."

# ==============================================================================
# VARIABLES SECTION
# Determines Type, Risk_level, and View dynamically based on logic.
# ==============================================================================

variables:
  - name: "Type"
    type: "string"
    initial_value: "Undetermined"
    rules:
      - condition: "AI_type == 3"
        value: "Non-regulated products"
      - condition: "AI_type == 1"
        value: "General-Purpose AI Model"
      - condition: "AI_type == 2"
        value: "AI System"
      - condition: "AI_type == 0"
        value: "Undetermined"
      - condition: "else"
        value: "Undetermined"

  - name: "Risk_level"
    type: "string"
    initial_value: "Undetermined"
    rules:
      # Group 1: Hard Stops
      - condition: "Totally_excluded == True"
        value: "Out of Regulation"
      - condition: "System_excluded == True"
        value: "Out of Regulation"
      - condition: "Unacceptable_risk == True"
        value: "Unacceptable Risk (Prohibited practices)"
      - condition: "AI_type == 3"
        value: "Out of Regulation"
      
      # Group 2: Real World Testing (RWT) overrides
      - condition: "Real_world_Testing == True and AI_type == 2 and High_risk == True and Transparency_obligation == True"
        value: "High Risk with Additional Transparency Obligation (Real World Testing)"
      - condition: "Real_world_Testing == True and AI_type == 2 and High_risk == True and Transparency_obligation == False"
        value: "High Risk (Real World Testing)"
      - condition: "Real_world_Testing == True and AI_type == 2 and High_risk == False and Transparency_obligation == True"
        value: "Transparency Obligation Only"
      - condition: "Real_world_Testing == True and AI_type == 2 and High_risk == False and Transparency_obligation == False and Open_source == True"
        value: "Minimal Risk (Open Source)"      
      - condition: "Real_world_Testing == True and AI_type == 2 and High_risk == False and Transparency_obligation == False and Open_source == False"
        value: "Minimal Risk"
      # Group 3: GPAI Market Placement
      - condition: "AI_type == 1 and Systemic_risk == True"
        value: "Systemic Risk GPAI"
      - condition: "AI_type == 1 and Systemic_risk == False and Open_source == False"
        value: "Standard GPAI"
      - condition: "AI_type == 1 and Systemic_risk == False and Open_source == True"
        value: "Standard GPAI (Open Source)"

      # Group 4: AI Systems Market Placement
      - condition: "AI_type == 2 and High_risk == True and Transparency_obligation == True"
        value: "High Risk with Additional Transparency Obligations"
      - condition: "AI_type == 2 and High_risk == True and Transparency_obligation == False"
        value: "High Risk"
      - condition: "AI_type == 2 and High_risk == False and Transparency_obligation == True"
        value: "Transparency Obligation Only"
      - condition: "AI_type == 2 and High_risk == False and Transparency_obligation == False and Open_source == True"
        value: "Minimal Risk (Open Source)"
      - condition: "AI_type == 2 and High_risk == False and Transparency_obligation == False and Open_source == False"
        value: "Minimal Risk"
      - condition: "Role == 'Undetermined'"
        value: "No Regulated Role Identified"
      # Fallback
      - condition: "else"
        value: "Undetermined"

  - name: "Risk_Level_Reason"
    type: "string"
    initial_value: "Reason undetermined."
    rules:
      # Group 1: Hard Stops (Extracts from Modules 1, 3, 4)
      - condition: "Totally_excluded == True"
        value: "This activity is excluded from the scope of the AI Act. Reason: {{Exclusion_reason}}" 
      - condition: "System_excluded == True"
        value: "This system is specifically exempt from the Regulation. Reason: {{Exclusion_Reason}}" 
      - condition: "Unacceptable_risk == True"
        value: "This AI practice is classified as 'Unacceptable Risk' and is strictly prohibited. Reason: {{Prohibited_reason}}" 
      - condition: "AI_type == 3"
        value: "This software falls outside the scope of the Regulation because it does not meet the legal definition of an 'AI System' or a 'General-Purpose AI Model' under AI Act Article 3."

      # Group 2: Real World Testing (RWT) overrides
      - condition: "Real_world_Testing == True and AI_type == 2 and High_risk == True"
        value: "You are conducting real-world testing on a High-Risk AI System. This high-risk status applies because {{High_risk_reason}} Therefore, the testing is subject to the governance requirements of AI Act Article 60."
      - condition: "Real_world_Testing == True and AI_type == 2 and High_risk == False and Transparency_obligation == True"
        value: "You are conducting real-world testing on a system that interacts with persons or generates content. This triggers transparency obligations because {{Transparency_Reasons}} You must comply with AI Act Article 50 even during testing."
      # Group 3: GPAI Market Placement (Extracts from Module 7)
      - condition: "AI_type == 1 and Systemic_risk == True"
        value: "This model is classified as a General-Purpose AI Model with Systemic Risk. Reason: {{Systemic_reason}}" 
      - condition: "AI_type == 1 and Systemic_risk == False"
        value: "This model is classified as a Standard General-Purpose AI Model. It does not meet the threshold for systemic risk because {{Systemic_reason}}"

      # Group 4: AI Systems Market Placement (Extracts from Module 5 & 6)
      - condition: "AI_type == 2 and High_risk == True and Transparency_obligation == True"
        value: "This system is classified as High-Risk because {{High_risk_reason}} Additionally, it is subject to transparency obligations because {{Transparency_Reasons}}"
      - condition: "AI_type == 2 and High_risk == True and Transparency_obligation == False"
        value: "This system is classified as High-Risk. Reason: {{High_risk_reason}}"
      - condition: "AI_type == 2 and High_risk == False and Transparency_obligation == True"
        value: "This system is classified as Limited Risk and is subject to transparency obligations. Reason: {{Transparency_Reasons}}"
      - condition: "AI_type == 2 and High_risk == False and Transparency_obligation == False and Open_source == False"
        value: "This system is classified as Minimal Risk because it meets none of the conditions for Unacceptable Risk, High-Risk, or Transparency Obligations defined in the Regulation."
      - condition: "AI_type == 2 and High_risk == False and Transparency_obligation == False and Open_source == True"
        value: "This system is classified as Minimal Risk (Open Source). It is released under a free and open-source licence and falls outside the scope of the Regulation, as it does not meet the criteria for Prohibited AI Practices, High-Risk AI Systems, or systems subject to Transparency Obligations."

      # Fallback
      - condition: "else"
        value: "A specific risk classification reason could not be determined from the provided inputs."

  - name: "Role_Description"
    type: "string"
    initial_value: "No regulated role identified based on current inputs."
    rules:
      - condition: "Role == 'provider'"
        value: "Per AI Act Article 3(3), you are the entity that develops an AI system or general-purpose AI model, or has it developed, and places it on the market or puts it into service under your own name or trademark."
      - condition: "Role == 'deployer'"
        value: "Per AI Act Article 3(4), you are the entity using an AI system under your authority for professional purposes, excluding personal non-professional activity."
      - condition: "Role == 'importer'"
        value: "Per AI Act Article 3(6), you are the entity located within the Union that places on the market an AI system bearing the name or trademark of a natural or legal person established in a third country."
      - condition: "Role == 'distributor'"
        value: "Per AI Act Article 3(7), you are the entity in the supply chain, other than the provider or importer, that makes an AI system available on the Union market."
      - condition: "Role == 'authorized_representative'"
        value: "Per AI Act Article 3(5), you are the entity located in the Union that has received and accepted a written mandate from a provider to perform specific obligations and procedures on their behalf."
      - condition: "Role == 'product_manufacturer'"
        value: "Per AI Act Article 2(1)(e), you are the manufacturer placing an AI system on the market or putting it into service together with your product and under your own name or trademark."
      - condition: "Role == 'Undetermined'"
        value: "The legal role of the entity remains undefined based on the provided inputs, preventing the assignment of specific liability or obligations under AI Act."

  - name: "Product_Type_Description"
    type: "string"
    initial_value: "Product type undetermined."
    rules:
      - condition: "Type == 'AI System'"
        value: "Per AI Act Article 3(1), this is a machine-based system designed to operate with varying levels of autonomy that infers from inputs how to generate outputs such as predictions, content, recommendations, or decisions influencing environments."
      - condition: "Type == 'General-Purpose AI Model'"
        value: "Per AI Act Article 3(63), this is an AI model that displays significant generality, is capable of competently performing a wide range of distinct tasks, and can be integrated into a variety of downstream systems or applications."
      - condition: "Type == 'Non-regulated products'"
        value: "Based on the user input, the software does not meet the definition of an AI system under AI Act Article 3(1) or a general-purpose AI model under AI Act Article 3(63) and is therefore outside the scope of the AI Act."
      - condition: "else"
        value: "Based on the user input, the specific category of the product remains undefined, preveting the determination of  theprecise scope of obligations under the AI Act."

  - name: "View"
    type: "string"
    initial_value: ""
    rules:
      # ------------------------------------------------------------------------
      # PRIORITY 1: HARD EXCLUSIONS (Regulation does not apply)
      # ------------------------------------------------------------------------
      - condition: "Totally_excluded == True"
        value: |
          This activity is classified as falling outside the regulated scope of the AI Act. The project is exempt from mandatory obligations, provided it remains within specific boundaries such as pure scientific research and development or pre-market research activities.
          For more details, please refer to AI Act Article 2(6) (Research Exemption), AI Act Article 2(8) (Pre-market R&D).

      - condition: "System_excluded == True"
        value: |
          This system qualifies for an exemption under AI Act Article 2. This applies if the system is exclusively for military, defence, or national security purposes, or if it is used by public authorities in a third country for international law enforcement cooperation under adequate safeguards. Note that "dual-use" systems (civilian and military) do not qualify for the military exemption.
          For more details, please refer to AI Act Article 2(3) (Military/National Security), AI Act Article 2(4) (Third Country Law Enforcement).

      - condition: "Type == 'Non-regulated products'"
        value: |
          The software meet the conditions of Exclusion or does not meet the legal definition of an 'AI System' or a 'General-Purpose AI Model'. Therefore, it falls outside the regulatory scope. You should strictly monitor the software's development to ensure it does not evolve into an AI system (e.g., by adding varying levels of autonomy or inference capabilities).
          For more details, please refer to AI Act Article 3(1) (Definition of AI System) , AI Act Article 3(63) (Definition of GPAI Model).

      # ------------------------------------------------------------------------
      # PRIORITY 2: PROHIBITIONS (Regulation bans the activity)
      # ------------------------------------------------------------------------
      - condition: "Unacceptable_risk == True"
        value: |
          This AI practice is strictly prohibited under AI Act Article 5. Banned practices include manipulative techniques, exploitation of vulnerabilities, social scoring, untargeted facial scraping, and emotion recognition in workplaces/schools. Placing this system on the market is forbidden for all operators, including providers, deployers, distributors, importers and so forth.
          For more details, please refer to AI Act Article 5 (Prohibited Practices).

      # ------------------------------------------------------------------------
      # PRIORITY 3: ROLE CHECK
      # ------------------------------------------------------------------------
      - condition: "Role == 'Undetermined'"
        value: |
          Your entity does not fit the definition of a regulated economic operator (Provider, Deployer, Importer, Distributor, or Authorized Representative). Review AI Act Article 25 to ensure you are not a 'Product Manufacturer' placing AI on the market under your name, which would reclassify you as a Provider.
          For more details, please refer to AI Act Article 3 (Definitions), AI Act Article 25 (Responsibilities along the AI value chain).

      # ------------------------------------------------------------------------
      # PRIORITY 3: REAL WORLD TESTING (EXPANDED)
      # ------------------------------------------------------------------------
      - condition: "(Risk_level == 'High Risk with Additional Transparency Obligation (Real World Testing)') and (Role == 'provider')"
        value: |
          As a provider, you must submit a real-world testing plan to the market surveillance authority, register the testing in the EU database, and obtain informed consent from subjects. You must also ensure the system is detectable as AI to affected persons.
          For more details, please refer to Article 13 (Instructions for Use), AI Act Article 49 (Registration Specifics), AI Act Article 50 (Transparency), AI Act Article 60 (Real World Testing), AI Act Article 61 (Informed Consent), AI Act Article 71 (EU database), AI Act Article 73 (Reporting of serious incidents) 

      - condition: "(Risk_level == 'High Risk with Additional Transparency Obligation (Real World Testing)') and (Role != 'provider')"
        value: |
          If you are a deployer, prospective deployer or partner. You must conclude an agreement with the provider specifying roles. You must ensure subjects are informed they are interacting with AI. If you are an Authorised Representative, you may need to facilitate registration.
          For more details, please refer to AI Act Article 13 (Instructions for Use), AI Act Article 60 (Real World Testing), AI Act Article 61 (Informed Consent), AI Act Article 50 (Transparency) and so forth.

      - condition: "(Risk_level == 'High Risk (Real World Testing)') and (Role == 'provider')"
        value: |
          As a provider, you must draw up and submit a testing plan to the market surveillance authority. You must register the testing in the EU database and ensure informed consent is obtained from all subjects.
          For more details, please refer to AI Act Article 13 (Instructions for Use), AI Act Article 49 (Registration Specifics), AI Act Article 60 (Real World Testing), AI Act Article 61 (Informed Consent), AI Act Article 71 (EU Database).

      - condition: "(Risk_level == 'High Risk (Real World Testing)') and (Role != 'provider')"
        value: |
          If you are a deployer, prospective deployer or partner, you must strictly adhere to the agreement concluded with the provider and ensure the instructions for use are followed. You must maintain oversight to protect subject safety.
          For more details, please refer to Article 13 (Instructions for Use), Article 26 (Obligations of Deployers), AI Act Article 60 (Real World Testing), AI Act Article 61 (Informed Consent).

      # ------------------------------------------------------------------------
      # PRIORITY 4: GENERAL-PURPOSE AI (GPAI)
      # ------------------------------------------------------------------------
      - condition: "(Risk_level == 'Systemic Risk GPAI' and Open_source == False) and (Role == 'provider')"
        value: |
          As a provider of a systemic risk model, you must notify the Commission and comply with all Article 53 obligations: maintain technical documentation, provide information to downstream providers, put in place a copyright policy, and publish a summary of training content. 
          Additionally, you must fulfill Article 55 obligations: perform adversarial testing, assess and mitigate systemic risks, report serious incidents to the AI Office , and ensure adequate cybersecurity.
          For more details, please refer to AI Act Article 52 (Notification), Article 53 (General Obligations), and Article 55 (Systemic Risk Obligations).

      - condition: "(Risk_level == 'Systemic Risk GPAI' and Open_source == False) and (Role != 'provider')"
        value: |
          Authorised Representatives (required for non-EU providers) must verify technical documentation and keep it available for 10 years. 
          Downstream Providers have the right to lodge complaints regarding infringements. If a downstream provider integrates this model into a high-risk AI system, they may assume provider obligations under Article 25.
          For more details, please refer to AI Act Article 54 (Authorised Reps) and Article 89 (Remedies).

      - condition: "(Risk_level == 'Systemic Risk GPAI' and Open_source == True) and (Role == 'provider')"
        value: |
          Because the model presents systemic risk, the standard open-source exemption for documentation does not apply. 
          You must comply with ALL provider obligations: draw up technical documentation, provide info to downstream providers, allow for copyright compliance, and publish a training summary. 
          You must also perform adversarial testing, mitigate systemic risks, and report incidents.
          For more details, please refer to AI Act Article 53(2) (Exception Override) and Article 55 (Systemic Risk Obligations).

      - condition: "(Risk_level == 'Systemic Risk GPAI' and Open_source == True) and (Role != 'provider')"
        value: |
          Authorised Representatives are required for non-EU providers because the model presents systemic risk (overriding the open-source exception for Reps). They must verify compliance and keep documentation for 10 years.
          Downstream Providers must receive full documentation from the provider and generally treat this as a regulated model.
          For more details, please refer to AI Act Article 54 (Authorised Rep).

      - condition: "(Risk_level == 'Standard GPAI') and (Role == 'provider')"
        value: |
          As a provider, you must draw up and keep up-to-date the technical documentation for the AI Office and national competent authorities. You must also draw up, keep up-to-date, and make available information and documentation to downstream providers who intend to integrate the model. Additionally, you must put in place a policy to comply with Union copyright law and make publicly available a sufficiently detailed summary of the content used for training the model. You are required to cooperate as necessary with the Commission and national competent authorities in the exercise of their competences. If you are established in a third country, you must appoint an authorised representative in the Union by written mandate.
          For more details, please refer to AI Act Article 53 (Obligations for GPAI Providers) and Article 54 (Authorised Representatives).

      - condition: "(Risk_level == 'Standard GPAI') and (Role != 'provider')"
        value: |
          Authorised Representatives must verify that the technical documentation has been drawn up and obligations fulfilled, keep a copy of the documentation at the disposal of the AI Office and national competent authorities for 10 years, provide necessary information to the AI Office upon a reasoned request, and cooperate with authorities in any action taken. They must terminate the mandate if they consider the provider is acting contrary to obligations and immediately inform the AI Office. Downstream providers have the right to lodge a complaint alleging an infringement of this Regulation.
          For more details, please refer to AI Act Article 54 (Authorised Representatives) and Article 89 (Remedies).

      - condition: "(Risk_level == 'Standard GPAI (Open Source)') and (Role == 'provider')"
        value: |
          If your model is released under a free and open-source licence that allows for access, usage, modification, and distribution, and its parameters, architecture, and usage information are made publicly available, you are exempt from the obligations to provide technical documentation to authorities and downstream providers. However, this exemption does not apply if the model presents systemic risk. You must still put in place a policy to comply with Union copyright law, publish a summary of the training content, and cooperate with the Commission and national competent authorities.
          For more details, please refer to AI Act Article 53(2) (Open Source Exemption) and Article 53(1)(c)-(d) (Copyright & Summary).

      - condition: "(Risk_level == 'Standard GPAI (Open Source)') and (Role != 'provider')"
        value: |
          The obligation to appoint an authorised representative does not apply to providers of open-source models defined in Article 53(2), unless the model presents systemic risks. Downstream providers should verify that the provider has put in place a policy to comply with Union copyright law and made the summary of training content available.
          For more details, please refer to AI Act Article 54(6) (Authorised Rep Exception Override).
      # ------------------------------------------------------------------------
      # PRIORITY 5: HIGH RISK & TRANSPARENCY
      # ------------------------------------------------------------------------
      - condition: "(Risk_level == 'High Risk with Additional Transparency Obligations') and (Role == 'provider')"
        value: |
          As a provider, you must fulfill all High-Risk obligations (Conformity Assessment, QMS, Registration). Additionally, you must design the system to inform natural persons they are interacting with AI, unless obvious from the context. If the system generates synthetic audio, image, video, or text, you must mark the outputs in a machine-readable format as artificially generated or manipulated.
          For more details, please refer to AI Act Article 16 (Provider Obligations), AI Act Article 50 (Transparency).

      - condition: "(Risk_level == 'High Risk with Additional Transparency Obligations') and (Role == 'deployer')"
        value: |
          As a Deployer, you must monitor the system and ensure human oversight. You are responsible for informing natural persons exposed to emotion recognition or biometric categorisation systems of their operation. If you deploy a system generating deep fakes, you must disclose that the content is artificially generated or manipulated. If you publish AI-generated text to inform the public on matters of public interest, you must disclose it is artificially generated. You must also inform workers and their representatives before putting a high-risk system into service at the workplace.
          For more details, please refer to AI Act Article 26 (Deployer Obligations), AI Act Article 50 (Transparency).

      - condition: "(Risk_level == 'High Risk with Additional Transparency Obligations') and ((Role != 'provider') and (Role != 'deployer'))"
        value: |
          Importers and Distributors must verify the system bears the required markings (including CE) and instructions before making it available. They must ensure storage or transport conditions do not jeopardize compliance. Authorised Representatives must verify the declaration of conformity and technical documentation. Product Manufacturers placing the AI under their name assume provider obligations.
          For more details, please refer to AI Act Article 23 (Importers), AI Act Article 24 (Distributors), AI Act Article 22 (Authorised Reps), AI Act Article 25 (Manufacturers).

      - condition: "(Risk_level == 'High Risk') and (Role == 'provider')"
        value: |
          As a provider, you must establish a Risk Management System, ensure Data Governance, and draw up Technical Documentation. You must have a Quality Management System, keep automatically generated logs, and undergo Conformity Assessment. You must register the system in the EU database and affix the CE marking. You must also establish a post-market monitoring system and report serious incidents to market surveillance authorities.
          For more details, please refer to AI Act Article 9 (Risk Management), AI Act Article 16 (Provider Obligations), AI Act Article 43 (Conformity), AI Act Article 49 (Registration).

      - condition: "(Risk_level == 'High Risk') and (Role == 'deployer')"
        value: |
          As a Deployer, you must use the system according to instructions, assign human oversight, and ensure input data is relevant and representative. You must monitor the system's operation and inform the provider of any risks or serious incidents. You must keep automatically generated logs for at least six months if under your control. Public authorities must register their use in the EU database. Certain deployers must conduct a Fundamental Rights Impact Assessment.
          For more details, please refer to AI Act Article 26 (Deployer Obligations), AI Act Article 27 (FRIA), AI Act Article 49 (Registration).

      - condition: "(Risk_level == 'High Risk') and ((Role != 'provider') and (Role != 'deployer'))"
        value: |
          Importers must ensure the provider has completed conformity assessment, drawn up technical documentation, and appointed a representative. Importers must keep a copy of the certificate and instructions for 10 years. Distributors must verify CE markings, declaration of conformity, and instructions. Both must ensure storage and transport do not jeopardize compliance. Authorised Representatives must keep documentation at the disposal of competent authorities and terminate their mandate if the provider acts contrary to obligations.
          For more details, please refer to AI Act Article 23 (Importers), AI Act Article 24 (Distributors), AI Act Article 22 (Authorised Reps).

      - condition: "(Risk_level == 'Transparency Obligation Only') and (Role == 'provider')"
        value: |
          As a Provider, you must ensure compliance with Article 50 by:
          1. Designing systems intended to interact directly with natural persons to inform them that they are interacting with an AI system, unless this is obvious from the context (Exception: Authorized law enforcement use for crime detection/investigation, unless available for public reporting).
          2. Ensuring systems generating synthetic audio, image, video, or text outputs are marked in a machine-readable format and are detectable as artificially generated.
          3. Ensuring these marking solutions are effective, interoperable, robust, and reliable (Exceptions to marking: Assistive functions for standard editing that do not substantially alter input data/semantics, or authorized law enforcement use).
          For more details, please refer to AI Act Article 50 (Transparency Obligations).

      - condition: "(Risk_level == 'Transparency Obligation Only') and (Role == 'deployer')"
        value: |
          As a Deployer, you must disclose the use of AI in the following scenarios:
          1. Emotion Recognition & Biometric Categorization: You must inform natural persons exposed to such systems of their operation (Exception: Authorized law enforcement use).
          2. Deep Fakes: You must disclose that image, audio, or video content generated or manipulated to resemble existing persons/events is artificially generated.
             - Artistic Exception: For evident artistic, creative, satirical, or fictional works, disclosure is limited to an appropriate manner that does not hamper enjoyment.
          3. Text of Public Interest: You must disclose that text published to inform the public on matters of public interest is artificially generated.
             - Editorial Exception: Disclosure is not required if the content has undergone human review/editorial control and a person holds editorial responsibility.
          4. General Requirements: Information must be clear, distinguishable, conform to accessibility requirements, and be provided at the latest at the time of the first interaction.
          For more details, please refer to AI Act Article 50 (Transparency Obligations).

      - condition: "(Risk_level == 'Transparency Obligation Only') and (Role != 'provider' and Role != 'deployer')"
        value: |
          As a Distributor, Importer, or other third party, you are subject to the following:
          1. Deemed Provider Status: You will be considered a "provider" and subject to all provider obligations (including Article 50 transparency) if you place the system on the market under your own name or trademark, make a substantial modification, or modify the intended purpose of the AI system.
          2. Verification: Importers must verify that the provider has drawn up the technical documentation and that the system is accompanied by instructions for use. Distributors must verify that the system is accompanied by instructions for use and that the provider and importer have complied with their obligations.
          For more details, please refer to AI Act Article 25 (Responsibilities along the AI value chain), Article 23 (Importers), Article 24 (Distributors), and Article 22 (Authorised Representatives).
      # ------------------------------------------------------------------------
      # PRIORITY 6: MINIMAL RISK
      # ------------------------------------------------------------------------
      - condition: "Risk_level == 'Minimal Risk'"
        value: |
          This system falls into the Minimal Risk category. The AI Act imposes no mandatory obligations. However, Providers and Deployers are encouraged to voluntarily adhere to Codes of Conduct.
          For more details, please refer to AI Act Article 95 (Codes of Conduct).
      
      - condition: "Risk_level == 'Minimal Risk (Open Source)'"
        value: |
          This system falls into the Minimal Risk category and is released under a free and open-source licence.
          Pursuant to AI Act Article 2(12), the Regulation does not apply to AI systems released under free and open-source licences unless they are high-risk, prohibited, or subject to transparency obligations. Therefore, this system is explicitly exempt from the Regulation's mandatory obligations.
          However, adherence to voluntary Codes of Conduct is still encouraged.
          For more details, please refer to AI Act Article 2 (Scope) and AI Act Article 95 (Codes of Conduct).

router:
  - condition: "True"
    action: "terminate"
    message: "Generate the report"
