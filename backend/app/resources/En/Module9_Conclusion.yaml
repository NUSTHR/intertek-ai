module_id: 9 #"09_compliance_conclusions"
title: "Final Compliance Determination"
description: "Maps parameter states to final legal conclusions and comprehensive obligation views based on Regulation (EU) 2024/1689."

# ==============================================================================
# VARIABLES SECTION
# Determines Type, Risk_level, and View dynamically based on logic.
# ==============================================================================

variables:
  - name: "Type"
    type: "string"
    initial_value: "Undetermined"
    rules:
      - condition: "AI_type == 3"
        value: "Non-regulated products"
      - condition: "AI_type == 1"
        value: "General-Purpose AI Model"
      - condition: "AI_type == 2"
        value: "AI System"
      - condition: "else"
        value: "Undetermined"

  - name: "Risk_level"
    type: "string"
    initial_value: "Undetermined"
    rules:
      # Group 1: Hard Stops
      - condition: "Totally_excluded == True"
        value: "Out of Regulation"
      - condition: "System_excluded == True"
        value: "Out of Regulation"
      - condition: "Unacceptable_risk == True"
        value: "Unacceptable Risk (Prohibited practices)"
      - condition: "AI_type == 3"
        value: "Out of Regulation"
      
      # Group 2: Real World Testing (RWT) overrides
      - condition: "Real_world_Testing == True and AI_type == 2 and High_risk == True and Transparency_obligation == True"
        value: "High Risk with Transparency Obligation (Real World Testing)"
      - condition: "Real_world_Testing == True and AI_type == 2 and High_risk == True and Transparency_obligation == False"
        value: "High Risk (Real World Testing)"
      - condition: "Real_world_Testing == True and AI_type == 2 and High_risk == False and Transparency_obligation == True"
        value: "Transparency Obligation Only (Real World Testing)"
      - condition: "Real_world_Testing == True and AI_type == 2 and High_risk == False and Transparency_obligation == False"
        value: "Minimal Risk (Real World Testing)"
      - condition: "Real_world_Testing == True and AI_type == 1"
        value: "Theoretically Exempt (Real-World Testing Inapplicable to GPAI Models)"

      # Group 3: GPAI Market Placement
      - condition: "AI_type == 1 and Systematic_risk == True and Open_source == False"
        value: "Systemic Risk"
      - condition: "AI_type == 1 and Systematic_risk == True and Open_source == True"
        value: "Systemic Risk (Open Source)"
      - condition: "AI_type == 1 and Systematic_risk == False and Open_source == False"
        value: "Standard GPAI"
      - condition: "AI_type == 1 and Systematic_risk == False and Open_source == True"
        value: "Standard GPAI (Open Source Exempt)"

      # Group 4: AI Systems Market Placement
      - condition: "AI_type == 2 and High_risk == True and Transparency_obligation == True"
        value: "High Risk with Transparency Obligation Obligations"
      - condition: "AI_type == 2 and High_risk == True and Transparency_obligation == False"
        value: "High Risk"
      - condition: "AI_type == 2 and High_risk == False and Transparency_obligation == True"
        value: "Transparency Obligation Only"
      - condition: "AI_type == 2 and High_risk == False and Transparency_obligation == False"
        value: "Minimal Risk"
      - condition: "Role == 'Undetermined'"
        value: "No Regulated Role Identified"
      # Fallback
      - condition: "else"
        value: "Undetermined"

  - name: "Risk_Level_Reason"
    type: "string"
    initial_value: "Reason undetermined."
    rules:
      # Group 1: Hard Stops (Extracts from Modules 1, 3, 4)
      - condition: "Totally_excluded == True"
        value: "{{Exclusion_reason}}" 
      - condition: "System_excluded == True"
        value: "{{Exclusion_Reason}}" # From Module 3 variable
      - condition: "Unacceptable_risk == True"
        value: "{{Prohibited_reason}}" # From Module 4 variable (implied need to ensure M4 exports this)
      - condition: "AI_type == 3"
        value: "The software does not meet the legal definition of an AI System or General-Purpose AI Model under Article 3."

      # Group 2: Real World Testing (RWT) overrides
      - condition: "Real_world_Testing == True and AI_type == 2 and High_risk == True"
        value: "Real-world testing of a High-Risk system (Reason: {{High_risk_reason}}) triggers Article 60 governance requirements."
      - condition: "Real_world_Testing == True and AI_type == 2 and High_risk == False and Transparency_obligation == True"
        value: "Real-world testing involves interaction with persons or content generation (Reason: {{Transparency_reason}}), triggering Article 50 transparency obligations."
      - condition: "Real_world_Testing == True and AI_type == 2 and High_risk == False and Transparency_obligation == False"
        value: "Real-world testing of minimal risk systems is not restricted by specific AI Act testing obligations."
      - condition: "Real_world_Testing == True and AI_type == 1"
        value: "Real-world testing rules (Article 60) apply to AI Systems, not raw GPAI models; public testing of a model constitutes placing it on the market."

      # Group 3: GPAI Market Placement (Extracts from Module 7)
      - condition: "AI_type == 1 and Systematic_risk == True"
        value: "{{Systematic_reason}}" # From Module 7 variable
      - condition: "AI_type == 1 and Systematic_risk == False"
        value: "The model does not meet the threshold for systemic risk (Reason: {{Systematic_reason}}) and is considered a Standard GPAI model."

      # Group 4: AI Systems Market Placement (Extracts from Module 5 & 6)
      - condition: "AI_type == 2 and High_risk == True and Transparency_obligation == True"
        value: "The system is High-Risk because {{High_risk_reason}}, and triggers transparency obligations because {{Transparency_reason}}."
      - condition: "AI_type == 2 and High_risk == True and Transparency_obligation == False"
        value: "{{High_risk_reason}}"
      - condition: "AI_type == 2 and High_risk == False and Transparency_obligation == True"
        value: "{{Transparency_reason}}"
      - condition: "AI_type == 2 and High_risk == False and Transparency_obligation == False"
        value: "The system does not fall under any High-Risk or Transparency categories defined in the Regulation."

      # Fallback
      - condition: "else"
        value: "Specific classification reason could not be determined from prior modules."

  - name: "Role_Description"
    type: "string"
    initial_value: "No regulated role identified based on current inputs."
    rules:
      - condition: "Role == 'provider'"
        value: "Per Article 3(3), you are the entity that develops an AI system or general-purpose AI model, or has it developed, and places it on the market or puts it into service under your own name or trademark."
      - condition: "Role == 'deployer'"
        value: "Per Article 3(4), you are the entity using an AI system under your authority for professional purposes, excluding personal non-professional activity."
      - condition: "Role == 'importer'"
        value: "Per Article 3(6), you are the entity located within the Union that places on the market an AI system bearing the name or trademark of a natural or legal person established in a third country."
      - condition: "Role == 'distributor'"
        value: "Per Article 3(7), you are the entity in the supply chain, other than the provider or importer, that makes an AI system available on the Union market."
      - condition: "Role == 'authorized_representative'"
        value: "Per Article 3(5), you are the entity located in the Union that has received and accepted a written mandate from a provider to perform specific obligations and procedures on their behalf."
      - condition: "Role == 'product_manufacturer'"
        value: "Per Article 2(1)(e), you are the manufacturer placing an AI system on the market or putting it into service together with your product and under your own name or trademark."
      - condition: "Role == 'Undetermined'"
        value: "The legal role of the entity remains undefined based on the provided inputs, preventing the assignment of specific liability or obligations under AI Act."

  - name: "Product_Type_Description"
    type: "string"
    initial_value: "Product type undetermined."
    rules:
      - condition: "Type == 'AI System'"
        value: "Per Article 3(1), this is a machine-based system designed to operate with varying levels of autonomy that infers from inputs how to generate outputs such as predictions, content, recommendations, or decisions influencing environments."
      - condition: "Type == 'General-Purpose AI Model'"
        value: "Per Article 3(63), this is an AI model that displays significant generality, is capable of competently performing a wide range of distinct tasks, and can be integrated into a variety of downstream systems or applications."
      - condition: "Type == 'Non-regulated products'"
        value: "The software does not meet the definition of an AI system under Article 3(1) or a general-purpose AI model under Article 3(63) and is therefore outside the scope of the AI Act."

  - name: "View"
    type: "string"
    initial_value: ""
    rules:
      # ------------------------------------------------------------------------
      # PRIORITY 1: HARD EXCLUSIONS (Regulation does not apply)
      # ------------------------------------------------------------------------
      - condition: "Totally_excluded == True"
        value: |
          This activity is classified as falling outside the regulated scope of the AI Act, meaning the project is currently exempt from the mandatory legal obligations, risk management standards, or regulatory oversight otherwise established by this Regulation.

      - condition: "System_excluded == True"
        value: |
          This system qualifies for a specific exemption under Article 2 due to its exclusive intended purpose for military, defense, or national security, or its use within the framework of international law enforcement cooperation. Consequently, the system is not subject to the requirements of the AI Act, provided its use remains strictly limited to these exempted purposes; any repurposing for civilian or commercial applications would immediately bring the system within the scope of the Regulation.

      - condition: "Type == 'Non-regulated products'"
        value: |
          The software does not meet the legal definition of an 'AI System' (Article 3(1)) or a 'General-Purpose AI Model' (Article 3(63)). Therefore, it falls outside the regulatory scope of the AI Act. You should strictly monitor the software's development to ensuring it does not evolve into an AI system, and continue to comply with applicable frameworks such as the General Product Safety Regulation and GDPR.

      # ------------------------------------------------------------------------
      # PRIORITY 2: PROHIBITIONS (Regulation bans the activity)
      # ------------------------------------------------------------------------
      - condition: "Unacceptable_risk == True"
        value: |
          This AI practice is classified as an Unacceptable Risk and is strictly prohibited under Article 5 of the AI Act. Placing this system on the market, putting it into service, or using it within the Union is forbidden and carries significant penalties. You must immediately cease this activity or fundamentally alter the system's design and intended purpose to eliminate prohibited capabilities such as manipulative techniques, social scoring, untargeted facial scraping, or biometric categorization of sensitive traits.

      # ------------------------------------------------------------------------
      # PRIORITY 3: ROLE CHECK (Cannot determine obligations without role)
      # ------------------------------------------------------------------------
      - condition: "Role == 'Undetermined'"
        value: |
          Based on the provided inputs, your entity does not currently fit the definition of a regulated economic operator (such as a Provider, Deployer, Importer, or Distributor) as defined in Article 3. Since the AI Act assigns legal obligations and liability strictly according to these specific roles, no compliance requirements can be determined at this stage. You are advised to re-examine Article 3 definitions to confirm your commercial relationship to the system, particularly if you plan to place it on the market or put it into service under your own name or trademark.

      # ------------------------------------------------------------------------
      # PRIORITY 4: REAL WORLD TESTING (Specific RWT Regimes)
      # ------------------------------------------------------------------------
      - condition: "Risk_level == 'High Risk with Transparency Obligation (Real World Testing)'"
        value: |
          You are conducting real-world testing of a High-Risk AI System that also involves direct interaction with natural persons. Compliance requires adherence to Article 60 for testing governance and Article 50 for transparency. You must submit a real-world testing plan to the Market Surveillance Authority, ensure test subjects provide informed consent, and explicitly disclose that they are interacting with an AI system. Additionally, you must implement effective oversight and ensure the immediate reversibility of any decisions made by the system during the testing phase.

      - condition: "Risk_level == 'High Risk (Real World Testing)'"
        value: |
          You are conducting real-world testing of a High-Risk AI System, which triggers the governance requirements of Article 60. You are legally required to submit a detailed testing plan to the Market Surveillance Authority in your Member State and obtain informed consent from all test subjects prior to participation. The testing environment must be subject to effective human oversight, and you must ensure that all personal data collected is deleted immediately once the testing concludes, unless specific retention conditions are met.

      - condition: "Risk_level == 'Transparency Obligation Only (Real World Testing)'"
        value: |
          While this testing phase does not require the submission of a formal testing plan under Article 60, the system involves interaction with natural persons or the generation of content, triggering the transparency obligations of Article 50. You must ensure that test subjects are clearly informed that they are interacting with an AI system or that the content they are viewing is artificially generated. Failure to provide this disclosure constitutes a breach of the Regulation even during the testing phase.

      - condition: "Risk_level == 'Minimal Risk (Real World Testing)'"
        value: |
          The AI Act does not impose specific regulatory restrictions on the real-world testing of Minimal Risk systems. You may proceed with testing without submitting a plan to the Market Surveillance Authority. However, you remain subject to general consumer protection laws and the GDPR if personal data is processed. You are encouraged to voluntarily adopt the ethical standards outlined in Article 95 to ensure the trustworthiness of your system during this phase.

      - condition: "Risk_level == 'Theoretically Exempt (Real-World Testing Inapplicable to GPAI Models)'"
        value: |
          The 'Real World Testing' regime (Article 60) applies exclusively to 'AI Systems,' not standalone 'General-Purpose AI Models.' Testing a raw model qualifies for the 'Pre-market R&D' exemption (Article 2(8)) unless it is integrated into a system, which triggers High-Risk rules. Critically, Making a model publicly available for testing constitutes 'placing on the market,' immediately voiding the R&D exemption and triggering full GPAI obligations.

      # ------------------------------------------------------------------------
      # PRIORITY 5: GPAI MARKET PLACEMENT
      # ------------------------------------------------------------------------
      - condition: "Risk_level == 'Systemic Risk'"
        value: |
          This model is classified as a General-Purpose AI Model with Systemic Risk under Article 51. Providers must strictly comply with the comprehensive obligations in Article 55, which include performing adversarial testing ('red-teaming') to identify vulnerabilities, assessing and mitigating systemic risks at the Union level, and ensuring high levels of cybersecurity. You are also required to document these processes in detail for the AI Office and report any serious incidents immediately.

      - condition: "Risk_level == 'Systemic Risk (Open Source)'"
        value: |
          Although this model is released under an open-source license, its classification as posing 'Systemic Risk' overrides the standard open-source exemptions in Article 53(2). You must fully comply with Article 55 obligations, including conducting adversarial evaluations, mitigating systemic risks, and reporting serious incidents to the AI Office. The transparency requirements regarding copyright policy and the publication of a training data summary also remain fully applicable.

      - condition: "Risk_level == 'Standard GPAI'"
        value: |
          This is a General-Purpose AI Model subject to the requirements of Article 53. Providers must draw up and maintain detailed technical documentation as specified in Annex XI and provide sufficient information to downstream providers (Annex XII) to enable their compliance. Additionally, you must put in place a policy to respect Union copyright law and publish a detailed summary of the content used to train the model.

      - condition: "Risk_level == 'Standard GPAI (Open Source Exempt)'"
        value: |
          As a non-systemic General-Purpose AI Model released under a free and open-source license, this model qualifies for the exemption in Article 53(2). You are not required to maintain Annex XI technical documentation or provide downstream information. However, you must strictly adhere to the remaining obligations: implementing a policy to respect Union copyright law and publishing a detailed summary of the content used for training the model.

      # ------------------------------------------------------------------------
      # PRIORITY 6: AI SYSTEMS MARKET PLACEMENT
      # ------------------------------------------------------------------------
      - condition: "Risk_level == 'High Risk with Transparency Obligation Obligations'"
        value: |
          This system is classified as High-Risk and also triggers specific transparency requirements. Providers must comply with the full conformity regime of Chapter III, including establishing a Risk Management System (Art 9), ensuring Data Governance (Art 10), and undergoing Conformity Assessment (Art 43). Simultaneously, you must satisfy Article 50 by ensuring that the system is designed to inform natural persons that they are interacting with an AI or that the outputs are artificially generated.

      - condition: "Risk_level == 'High Risk'"
        value: |
          This system is classified as High-Risk under Article 6. Providers must strictly adhere to the obligations defined in Chapter III before placing the system on the market. This includes establishing a continuous risk management system (Art 9), ensuring training data is high-quality and unbiased (Art 10), maintaining comprehensive technical documentation (Annex IV), ensuring human oversight capabilities (Art 14), and completing the appropriate conformity assessment procedure (Art 43).

      - condition: "Risk_level == 'Transparency Obligation Only'"
        value: |
          This system falls under the Limited Risk category, which is governed primarily by the transparency obligations of Article 50. While exempt from the heavy compliance burden of High-Risk systems, you must ensure that users are explicitly informed of the AI's nature. Depending on the system's function, this includes disclosing that they are interacting with a chatbot, that content is deep-faked, or that they are being exposed to emotion recognition technology.

      - condition: "Risk_level == 'Minimal Risk'"
        value: |
          This system is classified as Minimal Risk and is not subject to the mandatory requirements of the AI Act. You may place the system on the market without a conformity assessment. However, you remain subject to existing national and EU laws such as the GDPR and the General Product Safety Regulation. To foster trust, you are encouraged to voluntarily apply the requirements for trustworthy AI through Codes of Conduct as described in Article 95.

      # ------------------------------------------------------------------------
      # FALLBACK
      # ------------------------------------------------------------------------
      - condition: "else"
        value: |
          The compliance logic could not determine a definitive category based on the provided inputs. You should systematically review the inputs provided in Module 2 (Classification) and Module 5 (High-Risk) to ensure that the system's features and your entity's role are correctly identified. Without a clear classification, it is impossible to determine the specific legal obligations applicable to your project.

router:
  - condition: "True"
    action: "terminate"
    message: "Generate the report"